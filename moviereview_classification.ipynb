{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1672e9c4-0d80-4a97-a41e-b5dde796a7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/jupyter/.kaggle/kaggle.json'\n",
      "Downloading imdb-dataset-of-50k-movie-reviews.zip to /home/jupyter\n",
      " 35%|█████████████▎                        | 9.00M/25.7M [00:00<00:00, 85.9MB/s]\n",
      "100%|███████████████████████████████████████| 25.7M/25.7M [00:00<00:00, 164MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc94a53b-8049-4321-a1eb-075bd4494d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_zip_file = '/home/jupyter/imdb-dataset-of-50k-movie-reviews.zip'\n",
    "directory_to_extract_to = '/home/jupyter/dataset'\n",
    "\n",
    "import zipfile\n",
    "with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
    "    zip_ref.extractall(directory_to_extract_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "44b55579-afff-41c1-ab66-b477856bafcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "imdb_data = pd.read_csv('./dataset/IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "eed71cb5-711c-415f-8761-6a1bde80d5b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      One of the other reviewers has mentioned that ...  positive\n",
       "1      A wonderful little production. <br /><br />The...  positive\n",
       "2      I thought this was a wonderful way to spend ti...  positive\n",
       "3      Basically there's a family where a little boy ...  negative\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  I thought this movie did a down right good job...  positive\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  I'm going to have to disagree with the previou...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ac9c14-3da9-4028-89b9-5045edeec666",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0cbc2f05-dad9-44cf-ac72-fbec2288b77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        positive\n",
      "1        positive\n",
      "2        positive\n",
      "3        negative\n",
      "4        positive\n",
      "           ...   \n",
      "49995    positive\n",
      "49996    negative\n",
      "49997    negative\n",
      "49998    negative\n",
      "49999    negative\n",
      "Name: sentiment, Length: 50000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(imdb_data.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "dd937370-9725-4ee7-85f5-0c903d3887cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        0\n",
      "1        0\n",
      "2        0\n",
      "3        1\n",
      "4        0\n",
      "        ..\n",
      "49995    0\n",
      "49996    1\n",
      "49997    1\n",
      "49998    1\n",
      "49999    1\n",
      "Name: sentiment, Length: 50000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "imdb_data.sentiment = imdb_data.sentiment.apply(lambda x: 0 if x == 'positive' else 1)\n",
    "print(imdb_data.sentiment)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8fa67c73-b0c2-4ac9-8f76-02507c89f7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 124252 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "maxlen = 100\n",
    "max_words = 10000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(imdb_data.review)\n",
    "sequences = tokenizer.texts_to_sequences(imdb_data.review)\n",
    "print('Found %s unique tokens.' % len(tokenizer.word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "# #Tokenization of text\n",
    "# tokenizer=ToktokTokenizer()\n",
    "# #Setting English stopwords\n",
    "# stopword_list=nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9bebc108-9bb1-4663-bc9f-80708b1c5394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 100) (40000,)\n",
      "(10000, 100) (10000,)\n"
     ]
    }
   ],
   "source": [
    "X_train = data[:40000]\n",
    "y_train = imdb_data.sentiment[:40000]\n",
    "\n",
    "X_test = data[40000:]\n",
    "y_test = imdb_data.sentiment[40000:]\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "47936f33-f84e-4f61-b86c-543c17ca9c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first train review : [ 123  210 3241   68   14   34 1637    9   13 2239   10  413  131   10\n",
      "   13 1592   15    9   18   14   10  287   51   10 1417    3 1280   15\n",
      " 3184    2  189    5    1  299 2046    4 2150  570   21   39  570   18\n",
      " 7658 7154 5010   26 2983   41   15    3 6904  504   20  642    2   76\n",
      "  243   16    9   69 7598  651  710 6904  109  662   82 1208  693    5\n",
      "   65  574    4  920 2021   38 1208  559  147 3184   22  200  426 3819\n",
      "   16   48    6 3314  805 1603   43   22   67   76    8 1228   16  125\n",
      " 4103  486]\n",
      "first train sentiment : 0\n"
     ]
    }
   ],
   "source": [
    "print('first train review :', X_train[0])\n",
    "print('first train sentiment :', y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9f80282c-ed4a-4aa9-902b-d5feec137cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of each label\n",
      "0: 19993\n",
      "1: 20007\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Frequency of each label\")\n",
    "unique_elements, counts_elements = np.unique(y_train, return_counts=True)\n",
    "for unique, counts in zip(unique_elements, counts_elements):\n",
    "    print(\"%s: %d\" % (unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "52b79899-f7cc-4dd1-894a-3a4a978ad943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    1\n",
      "4    0\n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b63b25f7-9552-4a42-b2b8-c4344ab34e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "y_train = np.asarray(y_train, dtype='float32')\n",
    "y_test = np.asarray(y_test, dtype='float32')\n",
    "\n",
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c5c481-937d-4d0f-97a3-71e4e5d41ac3",
   "metadata": {},
   "source": [
    "## CNN 분류 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5b612b1a-a3ae-4670-b88c-144b64bc8903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-21 06:41:14.813232: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1247/1250 [============================>.] - ETA: 0s - loss: 0.3868 - acc: 0.8166\n",
      "Epoch 1: val_acc improved from -inf to 0.87550, saving model to best_model.h5\n",
      "1250/1250 [==============================] - 18s 5ms/step - loss: 0.3867 - acc: 0.8167 - val_loss: 0.2925 - val_acc: 0.8755\n",
      "Epoch 2/20\n",
      "1244/1250 [============================>.] - ETA: 0s - loss: 0.2243 - acc: 0.9107\n",
      "Epoch 2: val_acc improved from 0.87550 to 0.87760, saving model to best_model.h5\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2242 - acc: 0.9107 - val_loss: 0.2918 - val_acc: 0.8776\n",
      "Epoch 3/20\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 0.1182 - acc: 0.9574\n",
      "Epoch 3: val_acc did not improve from 0.87760\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.1182 - acc: 0.9574 - val_loss: 0.3516 - val_acc: 0.8756\n",
      "Epoch 4/20\n",
      "1242/1250 [============================>.] - ETA: 0s - loss: 0.0628 - acc: 0.9775\n",
      "Epoch 4: val_acc did not improve from 0.87760\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.0631 - acc: 0.9775 - val_loss: 0.4559 - val_acc: 0.8721\n",
      "Epoch 5/20\n",
      "1247/1250 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9857\n",
      "Epoch 5: val_acc did not improve from 0.87760\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.0403 - acc: 0.9857 - val_loss: 0.5470 - val_acc: 0.8669\n",
      "Epoch 5: early stopping\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dropout, Conv1D, GlobalMaxPooling1D, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "embedding_dim = 256 # 임베딩 벡터의 차원\n",
    "dropout_ratio = 0.3 # 드롭아웃 비율\n",
    "num_filters = 256 # 커널의 수\n",
    "kernel_size = 3 # 커널의 크기\n",
    "hidden_units = 128 # 뉴런의 수\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, embedding_dim))\n",
    "model.add(Dropout(dropout_ratio))\n",
    "model.add(Conv1D(num_filters, kernel_size, padding='valid', activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(hidden_units, activation='relu'))\n",
    "model.add(Dropout(dropout_ratio))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test), callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6950b062-f352-4271-85fa-558afe534f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2918 - acc: 0.8776\n",
      "\n",
      " 테스트 정확도: 0.8776\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load_model('best_model.h5')\n",
    "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf06d10-afc8-4527-9477-7aca79a13259",
   "metadata": {},
   "source": [
    "## RNN 분류 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c7177f3f-2516-415f-aeea-11320d2d31e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "496/500 [============================>.] - ETA: 0s - loss: 0.4552 - acc: 0.7990\n",
      "Epoch 1: val_acc improved from -inf to 0.85688, saving model to GRU_model.h5\n",
      "500/500 [==============================] - 6s 8ms/step - loss: 0.4541 - acc: 0.7997 - val_loss: 0.3374 - val_acc: 0.8569\n",
      "Epoch 2/15\n",
      "495/500 [============================>.] - ETA: 0s - loss: 0.2851 - acc: 0.8840\n",
      "Epoch 2: val_acc improved from 0.85688 to 0.86862, saving model to GRU_model.h5\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.2849 - acc: 0.8840 - val_loss: 0.3067 - val_acc: 0.8686\n",
      "Epoch 3/15\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.2297 - acc: 0.9104\n",
      "Epoch 3: val_acc improved from 0.86862 to 0.87650, saving model to GRU_model.h5\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.2297 - acc: 0.9104 - val_loss: 0.2962 - val_acc: 0.8765\n",
      "Epoch 4/15\n",
      "495/500 [============================>.] - ETA: 0s - loss: 0.1929 - acc: 0.9264\n",
      "Epoch 4: val_acc did not improve from 0.87650\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 0.1933 - acc: 0.9262 - val_loss: 0.3020 - val_acc: 0.8754\n",
      "Epoch 5/15\n",
      "495/500 [============================>.] - ETA: 0s - loss: 0.1644 - acc: 0.9389\n",
      "Epoch 5: val_acc did not improve from 0.87650\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 0.1644 - acc: 0.9389 - val_loss: 0.3471 - val_acc: 0.8694\n",
      "Epoch 6/15\n",
      "496/500 [============================>.] - ETA: 0s - loss: 0.1407 - acc: 0.9493\n",
      "Epoch 6: val_acc did not improve from 0.87650\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 0.1407 - acc: 0.9492 - val_loss: 0.3441 - val_acc: 0.8673\n",
      "Epoch 7/15\n",
      "495/500 [============================>.] - ETA: 0s - loss: 0.1191 - acc: 0.9565\n",
      "Epoch 7: val_acc did not improve from 0.87650\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 0.1196 - acc: 0.9562 - val_loss: 0.3662 - val_acc: 0.8611\n",
      "Epoch 7: early stopping\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GRU, Embedding\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "embedding_dim = 100\n",
    "hidden_units = 128\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, embedding_dim))\n",
    "model.add(GRU(hidden_units))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('GRU_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "51bc6da5-da88-44a7-8f2d-d5b6cb795e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 4ms/step - loss: 0.2990 - acc: 0.8729\n",
      "\n",
      " 테스트 정확도: 0.8729\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load_model('GRU_model.h5')\n",
    "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948759a6-e7a1-4fb2-8c70-fa9cdfb140b0",
   "metadata": {},
   "source": [
    "## CNN 분류 모델 (optimizer 변경)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a4f96f8-079f-436b-a6d7-124691264de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1245/1250 [============================>.] - ETA: 0s - loss: 0.4244 - acc: 0.7980\n",
      "Epoch 1: val_loss improved from inf to 0.32173, saving model to improved_cnn_model0.h5\n",
      "1250/1250 [==============================] - 10s 8ms/step - loss: 0.4240 - acc: 0.7983 - val_loss: 0.3217 - val_acc: 0.8586 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 0.2765 - acc: 0.8875\n",
      "Epoch 2: val_loss improved from 0.32173 to 0.29827, saving model to improved_cnn_model0.h5\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.2765 - acc: 0.8875 - val_loss: 0.2983 - val_acc: 0.8703 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "1247/1250 [============================>.] - ETA: 0s - loss: 0.1936 - acc: 0.9263\n",
      "Epoch 3: val_loss did not improve from 0.29827\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.1934 - acc: 0.9263 - val_loss: 0.3662 - val_acc: 0.8680 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 0.1283 - acc: 0.9532\n",
      "Epoch 4: val_loss did not improve from 0.29827\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.1283 - acc: 0.9532 - val_loss: 0.3803 - val_acc: 0.8705 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 0.0841 - acc: 0.9709\n",
      "Epoch 5: val_loss did not improve from 0.29827\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.0842 - acc: 0.9708 - val_loss: 0.5118 - val_acc: 0.8618 - lr: 0.0010\n",
      "Epoch 5: early stopping\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dropout, Conv1D, GlobalMaxPooling1D, Dense, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "embedding_dim = 256 # 임베딩 벡터의 차원\n",
    "dropout_ratio = 0.3 # 드롭아웃 비율\n",
    "num_filters = 256 # 커널의 수\n",
    "kernel_size = 3 # 커널의 크기\n",
    "hidden_units = 128 # 뉴런의 수\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, embedding_dim))\n",
    "model.add(Dropout(dropout_ratio))\n",
    "model.add(Conv1D(num_filters, kernel_size, padding='valid', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(num_filters, kernel_size, padding='valid', activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(hidden_units, activation='relu'))\n",
    "model.add(Dropout(dropout_ratio))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
    "lr_anneal = ReduceLROnPlateau(monitor = 'val_acc', patience=5, factor=0.2, min_lr=1e-6)\n",
    "mc = ModelCheckpoint('improved_cnn_model0.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test), callbacks=[es, mc, lr_anneal])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae534efc-fb03-4721-946e-b5ff3267eca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2983 - acc: 0.8703\n",
      "\n",
      " 테스트 정확도: 0.8703\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load_model('improved_cnn_model0.h5')\n",
    "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "43846b89-5181-4400-a5ab-af50e3796e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 0.3903 - acc: 0.8202\n",
      "Epoch 1: val_loss improved from inf to 0.30564, saving model to improved_cnn_model12.h5\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 0.3903 - acc: 0.8202 - val_loss: 0.3056 - val_acc: 0.8694 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "1244/1250 [============================>.] - ETA: 0s - loss: 0.2337 - acc: 0.9056\n",
      "Epoch 2: val_loss did not improve from 0.30564\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 0.2335 - acc: 0.9057 - val_loss: 0.3058 - val_acc: 0.8703 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "1246/1250 [============================>.] - ETA: 0s - loss: 0.1432 - acc: 0.9491\n",
      "Epoch 3: val_loss did not improve from 0.30564\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 0.1432 - acc: 0.9490 - val_loss: 0.3317 - val_acc: 0.8715 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "1243/1250 [============================>.] - ETA: 0s - loss: 0.0734 - acc: 0.9787\n",
      "Epoch 4: val_loss did not improve from 0.30564\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 0.0734 - acc: 0.9786 - val_loss: 0.3911 - val_acc: 0.8686 - lr: 0.0010\n",
      "Epoch 4: early stopping\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dropout, Conv1D, GlobalMaxPooling1D, Dense, BatchNormalization, Reshape, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam, Nadam\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "embedding_dim = 100 # 임베딩 벡터의 차원\n",
    "dropout_ratio = 0.3 # 드롭아웃 비율\n",
    "num_filters = 256 # 커널의 수\n",
    "kernel_size = 3 # 커널의 크기\n",
    "hidden_units = 128 # 뉴런의 수\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, embedding_dim))\n",
    "model.add(Conv1D(num_filters, kernel_size, padding='valid', activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
    "lr_anneal = ReduceLROnPlateau(monitor = 'val_acc', patience=5, factor=0.2, min_lr=1e-6)\n",
    "mc = ModelCheckpoint('improved_cnn_model12.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "\n",
    "model.compile(optimizer=Nadam(learning_rate=0.001), loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test), callbacks=[es, mc, lr_anneal])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0c95bdaa-25f0-4c6a-b76d-f2f441fc8bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.3091 - acc: 0.8714\n",
      "\n",
      " 테스트 정확도: 0.8714\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load_model('improved_cnn_model1.h5')\n",
    "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f059d6cb-de7e-4649-a12a-e0f6f6fe3480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3056 - acc: 0.8694\n",
      "\n",
      " 테스트 정확도: 0.8694\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load_model('improved_cnn_model12.h5')\n",
    "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d3eef85b-23db-405b-90c3-8d9968cd8558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1246/1250 [============================>.] - ETA: 0s - loss: 0.4536 - acc: 0.7884\n",
      "Epoch 1: val_loss improved from inf to 0.31900, saving model to improved_cnn_model11.h5\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 0.4534 - acc: 0.7885 - val_loss: 0.3190 - val_acc: 0.8621 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 0.2995 - acc: 0.8774\n",
      "Epoch 2: val_loss improved from 0.31900 to 0.31338, saving model to improved_cnn_model11.h5\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 0.2995 - acc: 0.8774 - val_loss: 0.3134 - val_acc: 0.8659 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 0.2208 - acc: 0.9158\n",
      "Epoch 3: val_loss did not improve from 0.31338\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 0.2208 - acc: 0.9158 - val_loss: 0.3210 - val_acc: 0.8743 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 0.1605 - acc: 0.9413\n",
      "Epoch 4: val_loss did not improve from 0.31338\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 0.1605 - acc: 0.9413 - val_loss: 0.3953 - val_acc: 0.8668 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 0.1118 - acc: 0.9588\n",
      "Epoch 5: val_loss did not improve from 0.31338\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 0.1118 - acc: 0.9589 - val_loss: 0.4668 - val_acc: 0.8602 - lr: 0.0010\n",
      "Epoch 5: early stopping\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dropout, Conv1D, GlobalMaxPooling1D, Dense, BatchNormalization, Reshape\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# embedding_dim = 100 # 임베딩 벡터의 차원\n",
    "# dropout_ratio = 0.3 # 드롭아웃 비율\n",
    "# num_filters = 256 # 커널의 수\n",
    "# kernel_size = 3 # 커널의 크기\n",
    "# hidden_units = 128 # 뉴런의 수\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(10000, embedding_dim))\n",
    "# model.add(Dropout(dropout_ratio))\n",
    "# model.add(Conv1D(num_filters, kernel_size, padding='valid', activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Conv1D(num_filters, kernel_size, padding='valid', activation='relu'))\n",
    "# #model.add(GlobalMaxPooling1D())\n",
    "# model.add(Dense(hidden_units, activation='relu'))\n",
    "# #model.add(Reshape((3,32)))\n",
    "# model.add(Dropout(dropout_ratio))\n",
    "# model.add(Conv1D(num_filters, kernel_size, padding='same', activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Conv1D(num_filters, kernel_size, padding='same', activation='relu'))\n",
    "# #model.add(GlobalMaxPooling1D())\n",
    "# model.add(Dense(hidden_units, activation='relu'))\n",
    "# #model.add(Reshape((3,32)))\n",
    "# model.add(Dropout(dropout_ratio))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "embedding_dim = 256 # 임베딩 벡터의 차원\n",
    "dropout_ratio = 0.3 # 드롭아웃 비율\n",
    "num_filters = 256 # 커널의 수\n",
    "kernel_size = 3 # 커널의 크기\n",
    "hidden_units = 128 # 뉴런의 수\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, embedding_dim))\n",
    "model.add(Dropout(dropout_ratio))\n",
    "model.add(Conv1D(num_filters, kernel_size, padding='valid', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(num_filters, kernel_size, padding='valid', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(num_filters, kernel_size, padding='valid', activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(hidden_units, activation='relu'))\n",
    "model.add(Dropout(dropout_ratio))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
    "lr_anneal = ReduceLROnPlateau(monitor = 'val_acc', patience=5, factor=0.2, min_lr=1e-6)\n",
    "mc = ModelCheckpoint('improved_cnn_model11.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test), callbacks=[es, mc, lr_anneal])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410ebfa8-06e8-4afc-94d6-473530b6328d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.3134 - acc: 0.8659\n",
      "\n",
      " 테스트 정확도: 0.8659\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load_model('improved_cnn_model11.h5')\n",
    "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4da60dbc-464d-4f4f-abd5-7a949b487cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 0.4279 - acc: 0.7985\n",
      "Epoch 1: val_acc improved from -inf to 0.83980, saving model to improved_cnn_model1.h5\n",
      "1250/1250 [==============================] - 10s 7ms/step - loss: 0.4279 - acc: 0.7985 - val_loss: 0.3508 - val_acc: 0.8398\n",
      "Epoch 2/20\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 0.2800 - acc: 0.8875\n",
      "Epoch 2: val_acc improved from 0.83980 to 0.87140, saving model to improved_cnn_model1.h5\n",
      "1250/1250 [==============================] - 9s 8ms/step - loss: 0.2802 - acc: 0.8875 - val_loss: 0.3091 - val_acc: 0.8714\n",
      "Epoch 3/20\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 0.2007 - acc: 0.9218\n",
      "Epoch 3: val_acc did not improve from 0.87140\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.2007 - acc: 0.9218 - val_loss: 0.3389 - val_acc: 0.8622\n",
      "Epoch 4/20\n",
      "1248/1250 [============================>.] - ETA: 0s - loss: 0.1355 - acc: 0.9498\n",
      "Epoch 4: val_acc did not improve from 0.87140\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.1355 - acc: 0.9498 - val_loss: 0.3845 - val_acc: 0.8659\n",
      "Epoch 5/20\n",
      "1246/1250 [============================>.] - ETA: 0s - loss: 0.0895 - acc: 0.9678\n",
      "Epoch 5: val_acc did not improve from 0.87140\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.0897 - acc: 0.9677 - val_loss: 0.4463 - val_acc: 0.8665\n",
      "Epoch 5: early stopping\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dropout, Conv1D, GlobalMaxPooling1D, Dense, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "embedding_dim = 256 # 임베딩 벡터의 차원\n",
    "dropout_ratio = 0.3 # 드롭아웃 비율\n",
    "num_filters = 256 # 커널의 수\n",
    "kernel_size = 3 # 커널의 크기\n",
    "hidden_units = 128 # 뉴런의 수\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, embedding_dim))\n",
    "model.add(Dropout(dropout_ratio))\n",
    "model.add(Conv1D(num_filters, kernel_size, padding='valid', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(num_filters, kernel_size, padding='valid', activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(hidden_units, activation='relu'))\n",
    "model.add(Dropout(dropout_ratio))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
    "#lr_anneal = ReduceLROnPlateau(monitor = 'val_acc', patience=5, factor=0.2, min_lr=1e-6)\n",
    "mc = ModelCheckpoint('improved_cnn_model1.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test), callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "624b39b2-7e5b-479a-a2a7-8a6cba16cc9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.3091 - acc: 0.8714\n",
      "\n",
      " 테스트 정확도: 0.8714\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load_model('improved_cnn_model1.h5')\n",
    "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "738d310f-6a7d-4643-893b-0b26a72619f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1241/1250 [============================>.] - ETA: 0s - loss: 0.3931 - acc: 0.8127\n",
      "Epoch 1: val_acc improved from -inf to 0.86770, saving model to improved_cnn_model3.h5\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.3930 - acc: 0.8128 - val_loss: 0.3066 - val_acc: 0.8677\n",
      "Epoch 2/20\n",
      "1245/1250 [============================>.] - ETA: 0s - loss: 0.2352 - acc: 0.9056\n",
      "Epoch 2: val_acc improved from 0.86770 to 0.87970, saving model to improved_cnn_model3.h5\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2349 - acc: 0.9058 - val_loss: 0.2875 - val_acc: 0.8797\n",
      "Epoch 3/20\n",
      "1241/1250 [============================>.] - ETA: 0s - loss: 0.1416 - acc: 0.9479\n",
      "Epoch 3: val_acc did not improve from 0.87970\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.1418 - acc: 0.9477 - val_loss: 0.3298 - val_acc: 0.8757\n",
      "Epoch 4/20\n",
      "1247/1250 [============================>.] - ETA: 0s - loss: 0.0753 - acc: 0.9738\n",
      "Epoch 4: val_acc did not improve from 0.87970\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.0753 - acc: 0.9738 - val_loss: 0.3994 - val_acc: 0.8707\n",
      "Epoch 5/20\n",
      "1242/1250 [============================>.] - ETA: 0s - loss: 0.0457 - acc: 0.9835\n",
      "Epoch 5: val_acc did not improve from 0.87970\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.0458 - acc: 0.9835 - val_loss: 0.4909 - val_acc: 0.8695\n",
      "Epoch 5: early stopping\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dropout, Conv1D, GlobalMaxPooling1D, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "embedding_dim = 256 # 임베딩 벡터의 차원\n",
    "dropout_ratio = 0.3 # 드롭아웃 비율\n",
    "num_filters = 256 # 커널의 수\n",
    "kernel_size = 3 # 커널의 크기\n",
    "hidden_units = 128 # 뉴런의 수\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, embedding_dim))\n",
    "model.add(Dropout(dropout_ratio))\n",
    "model.add(Conv1D(num_filters, kernel_size, padding='valid', activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(hidden_units, activation='relu'))\n",
    "model.add(Dropout(dropout_ratio))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
    "mc = ModelCheckpoint('improved_cnn_model3.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "model.compile(optimizer='nadam', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test), callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4fa2b410-2227-437f-88e1-ce8189874357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.2875 - acc: 0.8797\n",
      "\n",
      " 테스트 정확도: 0.8797\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load_model('improved_cnn_model3.h5')\n",
    "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eab8c3e5-b232-4035-a38e-21daa40d2d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1246/1250 [============================>.] - ETA: 0s - loss: 0.3756 - acc: 0.8236\n",
      "Epoch 1: val_acc improved from -inf to 0.86000, saving model to improved_cnn_model4.h5\n",
      "1250/1250 [==============================] - 8s 5ms/step - loss: 0.3753 - acc: 0.8238 - val_loss: 0.3233 - val_acc: 0.8600\n",
      "Epoch 2/20\n",
      "1243/1250 [============================>.] - ETA: 0s - loss: 0.2076 - acc: 0.9182\n",
      "Epoch 2: val_acc improved from 0.86000 to 0.86630, saving model to improved_cnn_model4.h5\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2075 - acc: 0.9183 - val_loss: 0.3256 - val_acc: 0.8663\n",
      "Epoch 3/20\n",
      "1246/1250 [============================>.] - ETA: 0s - loss: 0.0948 - acc: 0.9675\n",
      "Epoch 3: val_acc improved from 0.86630 to 0.87360, saving model to improved_cnn_model4.h5\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.0946 - acc: 0.9675 - val_loss: 0.3692 - val_acc: 0.8736\n",
      "Epoch 4/20\n",
      "1244/1250 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9877\n",
      "Epoch 4: val_acc did not improve from 0.87360\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.0382 - acc: 0.9876 - val_loss: 0.5169 - val_acc: 0.8625\n",
      "Epoch 5/20\n",
      "1244/1250 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9908\n",
      "Epoch 5: val_acc did not improve from 0.87360\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.0256 - acc: 0.9907 - val_loss: 0.6594 - val_acc: 0.8555\n",
      "Epoch 6/20\n",
      "1241/1250 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.9933\n",
      "Epoch 6: val_acc did not improve from 0.87360\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.0197 - acc: 0.9932 - val_loss: 0.6398 - val_acc: 0.8623\n",
      "Epoch 6: early stopping\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dropout, Conv1D, GlobalMaxPooling1D, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "embedding_dim = 256 # 임베딩 벡터의 차원\n",
    "dropout_ratio = 0.1 # 드롭아웃 비율\n",
    "num_filters = 256 # 커널의 수\n",
    "kernel_size = 3 # 커널의 크기\n",
    "hidden_units = 128 # 뉴런의 수\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, embedding_dim))\n",
    "model.add(Dropout(dropout_ratio))\n",
    "model.add(Conv1D(num_filters, kernel_size, padding='valid', activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(hidden_units, activation='relu'))\n",
    "model.add(Dropout(dropout_ratio))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "mc = ModelCheckpoint('improved_cnn_model4.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "model.compile(optimizer='nadam', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test), callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "224cd7e1-5eae-422b-8c6d-240b5de4511e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3692 - acc: 0.8736\n",
      "\n",
      " 테스트 정확도: 0.8736\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load_model('improved_cnn_model4.h5')\n",
    "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "487cace4-2fbd-4816-80bb-d47c72c611f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"conv1d_38\" is incompatible with the layer: expected min_ndim=3, found ndim=2. Full shape received: (None, 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10447/3558579280.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_filters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGlobalMaxPooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    226\u001b[0m       \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_ndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n\u001b[0m\u001b[1;32m    229\u001b[0m                          \u001b[0;34m'is incompatible with the layer: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                          \u001b[0;34mf'expected min_ndim={spec.min_ndim}, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"conv1d_38\" is incompatible with the layer: expected min_ndim=3, found ndim=2. Full shape received: (None, 16)"
     ]
    }
   ],
   "source": [
    "num_filters = 256 # 커널의 수\n",
    "kernel_size = 3 # 커널의 크기\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Conv1D(num_filters, kernel_size, padding='same', activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "mc = ModelCheckpoint('improved_cnn_model5.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=512, validation_data=(X_test, y_test), callbacks=[es, mc])\n",
    "results = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081e5637-c32d-4895-a200-42fcb52b9922",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bc3ecd9a-f134-4489-9b44-e242ac18399c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"conv1d_56\" is incompatible with the layer: expected min_ndim=3, found ndim=2. Full shape received: (None, 256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10447/1198594536.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_filters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGlobalMaxPooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_filters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGlobalMaxPooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    226\u001b[0m       \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_ndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n\u001b[0m\u001b[1;32m    229\u001b[0m                          \u001b[0;34m'is incompatible with the layer: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                          \u001b[0;34mf'expected min_ndim={spec.min_ndim}, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"conv1d_56\" is incompatible with the layer: expected min_ndim=3, found ndim=2. Full shape received: (None, 256)"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dropout, Conv1D, GlobalMaxPooling1D, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "embedding_dim = 256 # 임베딩 벡터의 차원\n",
    "dropout_ratio = 0.1 # 드롭아웃 비율\n",
    "num_filters = 256 # 커널의 수\n",
    "kernel_size = 3 # 커널의 크기\n",
    "hidden_units = 128 # 뉴런의 수\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, embedding_dim))\n",
    "model.add(Dropout(dropout_ratio))\n",
    "model.add(Conv1D(num_filters, kernel_size, padding='same', activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Conv1D(num_filters, 2, padding='same', activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(hidden_units, activation='relu'))\n",
    "model.add(Dropout(dropout_ratio))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "mc = ModelCheckpoint('improved_cnn_model5.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "model.compile(optimizer='nadam', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test), callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "762a964f-4ae7-4d63-b6d1-77326f1aa7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 860, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 919, in compute_loss\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/losses.py\", line 1932, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"/opt/conda/lib/python3.7/site-packages/keras/backend.py\", line 5247, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, 16) vs (None, 1)).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10447/1307544202.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mmc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'improved_cnn_model5.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'max'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 860, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 919, in compute_loss\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/losses.py\", line 1932, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"/opt/conda/lib/python3.7/site-packages/keras/backend.py\", line 5247, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, 16) vs (None, 1)).\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "input_layer = tf.keras.layers.Input(shape=(100,), dtype=tf.int32)\n",
    "\n",
    "layer = Embedding(10000, output_dim=embedding_dim)(input_layer)\n",
    "\n",
    "layer_conv3 = Conv1D(num_filters, 3, activation=\"relu\")(layer)\n",
    "layer_conv3 = GlobalMaxPooling1D()(layer_conv3)\n",
    "\n",
    "layer_conv4 = Conv1D(num_filters, 2, activation=\"relu\")(layer)\n",
    "layer_conv4 = GlobalMaxPooling1D()(layer_conv4)\n",
    "\n",
    "layer = tf.keras.layers.concatenate([layer_conv4, layer_conv3], axis=1)\n",
    "layer = BatchNormalization()(layer)\n",
    "layer = Dropout(0.3)(layer)\n",
    "\n",
    "output = Dense(16, activation=\"softmax\")(layer)\n",
    "\n",
    "model = tf.keras.Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "mc = ModelCheckpoint('improved_cnn_model5.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=512, validation_data=(X_test, y_test), callbacks=[es, mc])\n",
    "results = model.evaluate(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m100",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m100"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
