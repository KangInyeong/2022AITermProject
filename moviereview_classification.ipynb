{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1672e9c4-0d80-4a97-a41e-b5dde796a7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/jupyter/.kaggle/kaggle.json'\n",
      "Downloading imdb-dataset-of-50k-movie-reviews.zip to /home/jupyter\n",
      " 35%|█████████████▎                        | 9.00M/25.7M [00:00<00:00, 85.9MB/s]\n",
      "100%|███████████████████████████████████████| 25.7M/25.7M [00:00<00:00, 164MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc94a53b-8049-4321-a1eb-075bd4494d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_zip_file = '/home/jupyter/imdb-dataset-of-50k-movie-reviews.zip'\n",
    "directory_to_extract_to = '/home/jupyter/dataset'\n",
    "\n",
    "import zipfile\n",
    "with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
    "    zip_ref.extractall(directory_to_extract_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44b55579-afff-41c1-ab66-b477856bafcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "imdb_data = pd.read_csv('./dataset/IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eed71cb5-711c-415f-8761-6a1bde80d5b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      One of the other reviewers has mentioned that ...  positive\n",
       "1      A wonderful little production. <br /><br />The...  positive\n",
       "2      I thought this was a wonderful way to spend ti...  positive\n",
       "3      Basically there's a family where a little boy ...  negative\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  I thought this movie did a down right good job...  positive\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  I'm going to have to disagree with the previou...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ac9c14-3da9-4028-89b9-5045edeec666",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c6826d8-c6b4-4253-a5ec-ff969e5e7210",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import datasets\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8fa67c73-b0c2-4ac9-8f76-02507c89f7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 124252 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "maxlen = 100\n",
    "max_words = 10000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(imdb_data.review)\n",
    "sequences = tokenizer.texts_to_sequences(imdb_data.review)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "# #Tokenization of text\n",
    "# tokenizer=ToktokTokenizer()\n",
    "# #Setting English stopwords\n",
    "# stopword_list=nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9bebc108-9bb1-4663-bc9f-80708b1c5394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 100) (40000,)\n",
      "(10000, 100) (10000,)\n"
     ]
    }
   ],
   "source": [
    "X_train = data[:40000]\n",
    "y_train = imdb_data.sentiment[:40000]\n",
    "\n",
    "X_test = data[40000:]\n",
    "y_test = imdb_data.sentiment[40000:]\n",
    "print(X_train.shape,y_train.shape)\n",
    "print(X_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "47936f33-f84e-4f61-b86c-543c17ca9c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first train review : [ 123  210 3241   68   14   34 1637    9   13 2239   10  413  131   10\n",
      "   13 1592   15    9   18   14   10  287   51   10 1417    3 1280   15\n",
      " 3184    2  189    5    1  299 2046    4 2150  570   21   39  570   18\n",
      " 7658 7154 5010   26 2983   41   15    3 6904  504   20  642    2   76\n",
      "  243   16    9   69 7598  651  710 6904  109  662   82 1208  693    5\n",
      "   65  574    4  920 2021   38 1208  559  147 3184   22  200  426 3819\n",
      "   16   48    6 3314  805 1603   43   22   67   76    8 1228   16  125\n",
      " 4103  486]\n",
      "first train sentiment : positive\n"
     ]
    }
   ],
   "source": [
    "print('first train review :', X_train[0])\n",
    "print('first train sentiment :', y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9f80282c-ed4a-4aa9-902b-d5feec137cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of each label:\n",
      "[['negative' 'positive']\n",
      " [20007 19993]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "unique_elements, counts_elements = np.unique(y_train, return_counts=True)\n",
    "print(\"Frequency of each label:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "52b79899-f7cc-4dd1-894a-3a4a978ad943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    positive\n",
      "1    positive\n",
      "2    positive\n",
      "3    negative\n",
      "4    positive\n",
      "Name: sentiment, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0cbc2f05-dad9-44cf-ac72-fbec2288b77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        positive\n",
      "1        positive\n",
      "2        positive\n",
      "3        negative\n",
      "4        positive\n",
      "           ...   \n",
      "49995    positive\n",
      "49996    negative\n",
      "49997    negative\n",
      "49998    negative\n",
      "49999    negative\n",
      "Name: sentiment, Length: 50000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(imdb_data.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dd937370-9725-4ee7-85f5-0c903d3887cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        0\n",
      "1        0\n",
      "2        0\n",
      "3        1\n",
      "4        0\n",
      "        ..\n",
      "49995    0\n",
      "49996    1\n",
      "49997    1\n",
      "49998    1\n",
      "49999    1\n",
      "Name: sentiment, Length: 50000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(imdb_data.sentiment)):\n",
    "    if imdb_data.sentiment[i] == 'positive':\n",
    "        imdb_data.sentiment[i] = 0\n",
    "    else:\n",
    "        imdb_data.sentiment[i] = 1\n",
    "print(imdb_data.sentiment)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b63b25f7-9552-4a42-b2b8-c4344ab34e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "y_train = np.asarray(y_train).astype('float32')\n",
    "y_test = np.asarray(y_test).astype('float32')\n",
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c5c481-937d-4d0f-97a3-71e4e5d41ac3",
   "metadata": {},
   "source": [
    "## CNN 분류 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5b612b1a-a3ae-4670-b88c-144b64bc8903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-21 06:41:14.813232: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1247/1250 [============================>.] - ETA: 0s - loss: 0.3868 - acc: 0.8166\n",
      "Epoch 1: val_acc improved from -inf to 0.87550, saving model to best_model.h5\n",
      "1250/1250 [==============================] - 18s 5ms/step - loss: 0.3867 - acc: 0.8167 - val_loss: 0.2925 - val_acc: 0.8755\n",
      "Epoch 2/20\n",
      "1244/1250 [============================>.] - ETA: 0s - loss: 0.2243 - acc: 0.9107\n",
      "Epoch 2: val_acc improved from 0.87550 to 0.87760, saving model to best_model.h5\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2242 - acc: 0.9107 - val_loss: 0.2918 - val_acc: 0.8776\n",
      "Epoch 3/20\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 0.1182 - acc: 0.9574\n",
      "Epoch 3: val_acc did not improve from 0.87760\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.1182 - acc: 0.9574 - val_loss: 0.3516 - val_acc: 0.8756\n",
      "Epoch 4/20\n",
      "1242/1250 [============================>.] - ETA: 0s - loss: 0.0628 - acc: 0.9775\n",
      "Epoch 4: val_acc did not improve from 0.87760\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.0631 - acc: 0.9775 - val_loss: 0.4559 - val_acc: 0.8721\n",
      "Epoch 5/20\n",
      "1247/1250 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9857\n",
      "Epoch 5: val_acc did not improve from 0.87760\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.0403 - acc: 0.9857 - val_loss: 0.5470 - val_acc: 0.8669\n",
      "Epoch 5: early stopping\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dropout, Conv1D, GlobalMaxPooling1D, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "embedding_dim = 256 # 임베딩 벡터의 차원\n",
    "dropout_ratio = 0.3 # 드롭아웃 비율\n",
    "num_filters = 256 # 커널의 수\n",
    "kernel_size = 3 # 커널의 크기\n",
    "hidden_units = 128 # 뉴런의 수\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, embedding_dim))\n",
    "model.add(Dropout(dropout_ratio))\n",
    "model.add(Conv1D(num_filters, kernel_size, padding='valid', activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(hidden_units, activation='relu'))\n",
    "model.add(Dropout(dropout_ratio))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test), callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6950b062-f352-4271-85fa-558afe534f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2918 - acc: 0.8776\n",
      "\n",
      " 테스트 정확도: 0.8776\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load_model('best_model.h5')\n",
    "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf06d10-afc8-4527-9477-7aca79a13259",
   "metadata": {},
   "source": [
    "## RNN 분류 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c7177f3f-2516-415f-aeea-11320d2d31e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "496/500 [============================>.] - ETA: 0s - loss: 0.4552 - acc: 0.7990\n",
      "Epoch 1: val_acc improved from -inf to 0.85688, saving model to GRU_model.h5\n",
      "500/500 [==============================] - 6s 8ms/step - loss: 0.4541 - acc: 0.7997 - val_loss: 0.3374 - val_acc: 0.8569\n",
      "Epoch 2/15\n",
      "495/500 [============================>.] - ETA: 0s - loss: 0.2851 - acc: 0.8840\n",
      "Epoch 2: val_acc improved from 0.85688 to 0.86862, saving model to GRU_model.h5\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.2849 - acc: 0.8840 - val_loss: 0.3067 - val_acc: 0.8686\n",
      "Epoch 3/15\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.2297 - acc: 0.9104\n",
      "Epoch 3: val_acc improved from 0.86862 to 0.87650, saving model to GRU_model.h5\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.2297 - acc: 0.9104 - val_loss: 0.2962 - val_acc: 0.8765\n",
      "Epoch 4/15\n",
      "495/500 [============================>.] - ETA: 0s - loss: 0.1929 - acc: 0.9264\n",
      "Epoch 4: val_acc did not improve from 0.87650\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 0.1933 - acc: 0.9262 - val_loss: 0.3020 - val_acc: 0.8754\n",
      "Epoch 5/15\n",
      "495/500 [============================>.] - ETA: 0s - loss: 0.1644 - acc: 0.9389\n",
      "Epoch 5: val_acc did not improve from 0.87650\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 0.1644 - acc: 0.9389 - val_loss: 0.3471 - val_acc: 0.8694\n",
      "Epoch 6/15\n",
      "496/500 [============================>.] - ETA: 0s - loss: 0.1407 - acc: 0.9493\n",
      "Epoch 6: val_acc did not improve from 0.87650\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 0.1407 - acc: 0.9492 - val_loss: 0.3441 - val_acc: 0.8673\n",
      "Epoch 7/15\n",
      "495/500 [============================>.] - ETA: 0s - loss: 0.1191 - acc: 0.9565\n",
      "Epoch 7: val_acc did not improve from 0.87650\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 0.1196 - acc: 0.9562 - val_loss: 0.3662 - val_acc: 0.8611\n",
      "Epoch 7: early stopping\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GRU, Embedding\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "embedding_dim = 100\n",
    "hidden_units = 128\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, embedding_dim))\n",
    "model.add(GRU(hidden_units))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('GRU_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "51bc6da5-da88-44a7-8f2d-d5b6cb795e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 4ms/step - loss: 0.2990 - acc: 0.8729\n",
      "\n",
      " 테스트 정확도: 0.8729\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load_model('GRU_model.h5')\n",
    "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d867df1-c7b9-403a-9f69-a2dfaefbd3bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m100",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m100"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
